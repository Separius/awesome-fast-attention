# awesome-fast-attention [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of efficient attention modules (last update: {{{generation-date}}})

## Table of Contents

* **[Efficient Attention](#efficient-attention)**
* **[Articles/Surveys/Benchmarks](#articlessurveysbenchmarks)**

## Efficient Attention

{{{fast-attention-table}}}

## Articles/Surveys/Benchmarks

* [A Survey of Long-Term Context in Transformers](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/)
* [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

* [Long Range Arena: A Benchmark for Efficient
    Transformers](https://arxiv.org/abs/2011.04006)

