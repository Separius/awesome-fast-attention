[
  {
    "name": "T-DMCA",
    "arxiv_id": "1801.10198",
    "comment": "compresses key and value + blocked attention",
    "complexity": "{b}*\\frac{N}{b}*\\frac{N}{{b}*{k}}*{D}",
    "causal": false
  },
  {
    "name": "Criss Cross Attention",
    "arxiv_id": "1811.11721",
    "code": "https://github.com/speedinghzl/CCNet",
    "comment": "each pixel attends to its row and column simultaneously",
    "complexity": "{N}*({H}+{W})*{D}",
    "causal": false
  },
  {
    "name": "Efficient Attention",
    "arxiv_id": "1812.01243",
    "code": "https://github.com/cmsflash/efficient-attention",
    "comment": "Softmax(Q)*(Softmax(K^T)*V)",
    "complexity": "{N}*{D}^2",
    "causal": false
  },
  {
    "name": "Sparse Transformer",
    "arxiv_id": "1904.10509",
    "code": "https://github.com/ptillet/torch-blocksparse",
    "comment": "sparse block based attention",
    "complexity": "{N}*\\sqrt{N}*{D}",
    "causal": true
  },
  {
    "name": "GCNet",
    "arxiv_id": "1904.11492",
    "code": "https://github.com/xvjiarui/GCNet",
    "comment": "squeeze and excitation with an attention pooling (instead of a GAP)",
    "complexity": "{N}*{D}^2",
    "causal": false
  },
  {
    "name": "Interlaced Attention",
    "arxiv_id": "1907.12273",
    "code": "IN_PAPER",
    "comment": "combination of a short length and then long range(dilated) attention",
    "complexity": "{N}*{D}^2+{N}*\\sqrt{N}*{D}",
    "causal": true
  },
  {
    "name": "EMA",
    "arxiv_id": "1907.13426",
    "code": "https://github.com/XiaLiPKU/EMANet",
    "comment": "applys expectation maximization to cluster keys into k clusters",
    "complexity": "{N}*{k}*{D}",
    "causal": false
  },
  {
    "name": "BP-Transformer",
    "arxiv_id": "1911.04070",
    "code": "https://github.com/yzh119/BPT",
    "comment": "attends to distant tokens coarsely and attends to close tokens in a more fine-grained manner",
    "complexity": "{N}*{k}*\\log(\\frac{N}{k})*{D}",
    "causal": true
  },
  {
    "name": "Compressive Transformer",
    "arxiv_id": "1911.05507",
    "code": "https://github.com/lucidrains/compressive-transformer-pytorch",
    "comment": "compresses distant tokens instead of just stop_grad() ing them, more efficient version of transformerXL",
    "complexity": "{N}^2*{D}",
    "causal": true
  },
  {
    "name": "Axial Attention",
    "arxiv_id": "1912.12180",
    "code": "https://github.com/lucidrains/axial-attention",
    "comment": "apply attention on each axis separately",
    "complexity": "{N}*({H}+{W})*{D}",
    "causal": true
  },
  {
    "name": "Transformer on a Diet",
    "arxiv_id": "2002.06170",
    "code": "https://github.com/cgraywang/transformer-on-diet",
    "comment": "dilated transformer like wavenet",
    "complexity": "{N}*{k}*{D}",
    "causal": true
  },
  {
    "name": "Routing Transformer",
    "arxiv_id": "2003.05997",
    "code": "https://github.com/lucidrains/routing-transformer",
    "comment": "computes attention with same-cluster tokens (computed by online k-means)",
    "complexity": "{N}*\\sqrt{N}*{D}",
    "causal": false
  },
  {
    "name": "SAC",
    "arxiv_id": "2003.09833",
    "comment": "learns the q, k connections == dynamically creates a sparse attention matrix",
    "complexity": "{N}*{k}*{D}",
    "causal": true
  },
  {
    "name": "AutoNL",
    "arxiv_id": "2004.01961",
    "code": "https://github.com/LiYingwei/AutoNL",
    "comment": "computes Q(KV) and also down samples q, k, v both in spatial and channel dimensions",
    "complexity": "(\\frac{H}{h}*\\frac{W}{w})*(\\frac{D}{k})^2",
    "causal": false
  },
  {
    "name": "ETC",
    "arxiv_id": "2004.08483",
    "comment": "combines global attention (star transformer with multiple global tokens) with local attention",
    "complexity": "({N}*{g}+{g}^2+{N}*{k})*{D}",
    "causal": true
  },
  {
    "name": "Jukebox",
    "arxiv_id": "2005.00341",
    "code": "https://github.com/openai/jukebox",
    "comment": "better attention patterns from Sparse Transformer",
    "complexity": "{N}*\\sqrt{N}*{D}",
    "causal": true
  },
  {
    "name": "GMAT",
    "arxiv_id": "2006.03274",
    "code": "https://github.com/ag1988/gmat",
    "comment": "adds global tokens",
    "complexity": "{m}*({N}+{m})*{D}",
    "causal": false
  },
  {
    "name": "FANet",
    "arxiv_id": "2007.03815",
    "comment": "l2_norm(q)*(l2_norm(k)*v)",
    "complexity": "{N}*{D}^2",
    "causal": false
  },
  {
    "name": "Clustered Attention",
    "arxiv_id": "2007.04825",
    "code": "https://github.com/idiap/fast-transformers",
    "comment": "groups queries together with LSH",
    "complexity": "{N}*{k}*{D}",
    "causal": false
  }
]