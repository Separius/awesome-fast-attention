[AN ATTENTION FREE TRANSFORMER](https://openreview.net/pdf?id=pW--cu2FCHY)

[LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS](https://openreview.net/pdf?id=qVyeW-grC2k)

[LAMBDA NETWORKS: MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION](https://openreview.net/pdf?id=xTJEN-ggl1b)

[Memformer: The Memory-Augmented Transformer](https://arxiv.org/abs/2010.06891)

[SMYRF: Efficient Attention using Asymmetric Clustering](https://arxiv.org/abs/2010.05315)

[Is Attention better than matrix decomposition](https://openreview.net/pdf?id=1FvkSpWosOl)

[Sub-Linear Memory: How to Make Performers SLiM](https://arxiv.org/abs/2012.11346)

[Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902)
[Linear Transformers Are Secretly Fast Weight Memory Systems](https://arxiv.org/abs/2102.11174)
